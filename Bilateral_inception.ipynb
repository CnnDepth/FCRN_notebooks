{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, tqdm_notebook\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bilateral Inception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilateralFilter():\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Trainable variables: transformation matrix Lambda and scale parameter theta\n",
    "        self.Lambda = tf.Variable(np.eye(3), dtype=tf.float64, trainable=True)\n",
    "        self.theta = tf.Variable([1], dtype=tf.float64, trainable=True)\n",
    "        \n",
    "        \n",
    "    def __call__(self, z, features):\n",
    "        \"\"\"\n",
    "        Applies bilateral filter to \n",
    "        Input:\n",
    "        z: superpixels of predicted image - 3d tensor (batch_size, 1024, 1)\n",
    "        features: coordinates and depth of pixels of predicted image - 3d tensor (batch_size, 1024, 3)\n",
    "        \n",
    "        Output:\n",
    "        z_new: superpixels of filtered image - 3d tensor (batch_size, 1024, 1)\n",
    "        \"\"\"\n",
    "        # transform features\n",
    "        Lf = tf.einsum('ijk,kl->ijl', features, tf.transpose(self.Lambda))\n",
    "        lf1_tiled = tf.tile(tf.expand_dims(Lf, axis=1), [1, 1024, 1, 1])\n",
    "        lf2_tiled = tf.tile(tf.expand_dims(Lf, axis=2), [1, 1, 1024, 1])\n",
    "\n",
    "        # compute filter matrix K\n",
    "        D = tf.reduce_sum((lf1_tiled - lf2_tiled) ** 2, axis=-1) # D_ij = ||Lf_i - Lf_j||^2\n",
    "        theta_D = self.theta * D\n",
    "        K = tf.nn.softmax(theta_D, axis=2)\n",
    "        \n",
    "        # the result is transformation of z with matrix K\n",
    "        z_new = tf.einsum('ijk,ikl->ijl', K, z)\n",
    "        return z_new\n",
    "    \n",
    "    \n",
    "    def get_trainables(self):\n",
    "        return [self.Lambda, self.theta]\n",
    "        \n",
    "\n",
    "class BilateralInception():\n",
    "    \n",
    "    def __init__(self, n_filters):\n",
    "        self.n_filters = n_filters\n",
    "        self.weights = tf.Variable(np.ones(6) / 6., dtype=tf.float64)\n",
    "        self.filters = [BilateralFilter() for _ in range(self.n_filters)]\n",
    "        \n",
    "        \n",
    "    def __call__(self, z, features):\n",
    "        \"\"\"\n",
    "        Applies bilateral filter to \n",
    "        Input:\n",
    "        z: superpixels of predicted image - 3d tensor (batch_size, 1024, 1)\n",
    "        features: coordinates and depth of pixels of predicted image - 3d tensor (batch_size, 1024, 3)\n",
    "        \n",
    "        Output:\n",
    "        result: superpixels of filtered image - 3d tensor (batch_size, 1024, 1)\n",
    "        \"\"\"\n",
    "        filter_outputs = [tf.expand_dims(filtr(z, features), 0) for filtr in self.filters]\n",
    "        filter_outputs = tf.concat(filter_outputs, axis=0)\n",
    "        result = tf.reduce_sum(self.weights[:, tf.newaxis, tf.newaxis, tf.newaxis] * filter_outputs, axis=0)\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def get_trainables(self):\n",
    "        result = []\n",
    "        for filtr in self.filters:\n",
    "            result += filtr.get_trainables()\n",
    "        result.append(self.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Граф вычислений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_module = BilateralInception(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_plh = tf.placeholder(tf.float64, [None, 1024, 1])\n",
    "feature_plh = tf.placeholder(tf.float64, [None, 1024, 3])\n",
    "gt_plh = tf.placeholder(tf.float64, [None, 1024, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_filtered = bi_module(z_plh, feature_plh)\n",
    "loss = tf.losses.mean_squared_error(z_filtered, gt_plh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = h5py.File('/home/kmouraviev/NYU_dataset_hdf5/data.hdf5', 'r')\n",
    "rgbs = dataset['data']\n",
    "depths = dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgbs = np.array(rgbs)\n",
    "depths = np.array(depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids = np.random.choice(np.arange(len(rgbs)), size=1000, replace=False)\n",
    "rgbs_sample = rgbs[sample_ids]\n",
    "depths_sample = depths[sample_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgbs_train = rgbs[:40000]\n",
    "depths_train = depths[:40000]\n",
    "rgbs_val = rgbs[40000:]\n",
    "depths_val = depths[40000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('/home/kmouraviev/FCRN_notebooks/finetune_sgd_lr1e-05_decay1e-05/model_on_epoch1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths_predicted = model.predict(rgbs_train, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DEPTH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_superpixels(image):\n",
    "    superpixels = []\n",
    "    for i in range(0, image.shape[0], 7):\n",
    "        for j in range(0, image.shape[0], 7):\n",
    "            superpixels.append(image[i:i+7, j:j+7].mean() / MAX_DEPTH)\n",
    "    return np.array(superpixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image):\n",
    "    features = []\n",
    "    for i in range(0, image.shape[0], 7):\n",
    "        for j in range(0, image.shape[0], 7):\n",
    "            features.append([(i + 4) / image.shape[0], \\\n",
    "                             (j + 4) / image.shape[1], \\\n",
    "                             image[i:i+7, j:j+7].mean() / MAX_DEPTH])\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dcb5237e1d54b6b846de6b4c4a0d6ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=40000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b1666f2fde642f0b158c69113036799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=40000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79289665ad42439fa4f6f063b4dd2682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=40000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_superpixels = np.array([extract_superpixels(image) for image in tqdm_notebook(depths_predicted)])\n",
    "gt_superpixels = np.array([extract_superpixels(image) for image in tqdm_notebook(depths_train)])\n",
    "prediction_features = np.array([extract_features(image) for image in tqdm_notebook(depths_predicted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_superpixels = prediction_superpixels[:, :, np.newaxis]\n",
    "gt_superpixels = gt_superpixels[:, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 1024, 1) (40000, 1024, 1) (40000, 1024, 3)\n"
     ]
    }
   ],
   "source": [
    "print(prediction_superpixels.shape, gt_superpixels.shape, prediction_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4238108f22c43e885ba980c6f4a1685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "loss_history = []\n",
    "for i in tqdm_notebook(np.arange(50000)):\n",
    "    ids = np.random.choice(np.arange(len(depths_train)), size=BATCH_SIZE, replace=False)\n",
    "    features = prediction_features[ids]\n",
    "    gt = gt_superpixels[ids]\n",
    "    prediction = prediction_superpixels[ids]\n",
    "    feed_dict = {feature_plh: features,\n",
    "                z_plh: prediction,\n",
    "                gt_plh: gt\n",
    "               }\n",
    "    loss_value, _ = sess.run([loss, train_step], feed_dict=feed_dict)\n",
    "    loss_history.append(loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, n):\n",
    "    cumsum = np.cumsum(x)\n",
    "    return (cumsum[n:] - cumsum[:-n]) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x3ff5f4edff98>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(running_mean(loss_history, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка отфильтрованных изображений на валидационной выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as sps\n",
    "\n",
    "def apply_gaussian_filter(source_image,\n",
    "                          filtered_superpixels,\n",
    "                          sigma=7,\n",
    "                          k=5):\n",
    "    image = source_image.copy()\n",
    "    grid_x, grid_y = np.meshgrid(np.arange(-(k // 2), k // 2 + 1),\n",
    "                                 np.arange(-(k // 2), k // 2 + 1))\n",
    "    grid = np.array(list(zip(grid_x.ravel(), grid_y.ravel()))).reshape((k, k, 2))\n",
    "    gaussian_window = sps.multivariate_normal(mean=[0, 0], cov=np.eye(2) * sigma).pdf(grid)\n",
    "    gaussian_window /= gaussian_window.sum()\n",
    "    for n in range(1024):\n",
    "        i = (n // 32) * 7\n",
    "        j = (n % 32) * 7\n",
    "        image[i:i+7, j:j+7] = filtered_superpixels[n]\n",
    "    for i in range(k // 2, source_image.shape[0] - k // 2):\n",
    "        for j in range(k // 2, source_image.shape[1] - k // 2):\n",
    "            image[i, j] = np.sum(image[i - k // 2:i + k // 2 + 1, j - k // 2:j + k // 2 + 1] \\\n",
    "                                 * gaussian_window)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGMA = 7\n",
    "K = 5\n",
    "\n",
    "def apply_gaussian_filter_parallel(data):\n",
    "    image, superpixels = data\n",
    "    return apply_gaussian_filter(image, superpixels, sigma=SIGMA, k=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16541/16541 [==============================] - 55s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "depths_val_predicted = model.predict(rgbs_val, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a00bd98592a4a7a913f60eddcd4efbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16541), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b79b9275ed47ca8d041f3ef5048f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16541), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ee9907b71e4ffdad090d983dcbfc95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16541), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_superpixels = np.array([extract_superpixels(image) for image in tqdm_notebook(depths_val_predicted)])\n",
    "gt_superpixels = np.array([extract_superpixels(image) for image in tqdm_notebook(depths_val)])\n",
    "prediction_features = np.array([extract_features(image) for image in tqdm_notebook(depths_val_predicted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982691dcf0dd452d94f047278aa9dc07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=517), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "filtered_batches = []\n",
    "for i in tqdm_notebook(range(0, len(depths_val), BATCH_SIZE)):\n",
    "    superpixels_batch = prediction_superpixels[i:min(i + BATCH_SIZE, len(depths_val))]\n",
    "    features_batch = prediction_features[i:min(i + BATCH_SIZE, len(depths_val))]\n",
    "    filtered_batch = sess.run(z_filtered, feed_dict={feature_plh: features_batch,\n",
    "                                                    z_plh: superpixels_batch[:, :, np.newaxis]}\n",
    "                             )\n",
    "    filtered_batches.append(filtered_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16541, 1024, 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_superpixels = np.concatenate(filtered_batches, axis=0)\n",
    "filtered_superpixels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "pool = Pool(160)\n",
    "filtered_images = pool.map(apply_gaussian_filter_parallel,\n",
    "                           zip(depths_val / 10, filtered_superpixels)\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_images = np.array(filtered_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16541, 224, 224)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение качества на валидационной выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of raw images: 0.3616211401499641\n",
      "MSE of filtered images: 0.34023888637559724\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE of raw images:\", np.mean((depths_val_predicted - depths_val) ** 2))\n",
    "print(\"MSE of filtered images:\", np.mean((filtered_images * 10 - depths_val) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-MSE of raw images: 0.08970439863487457\n",
      "log-MSE of filtered images: 0.07114004677877604\n"
     ]
    }
   ],
   "source": [
    "print(\"log-MSE of raw images:\", np.mean((np.log(depths_val_predicted + 1e-2) - np.log(depths_val + 1e-2)) ** 2))\n",
    "print(\"log-MSE of filtered images:\", \n",
    "      np.mean((np.log(filtered_images * 10 + 1e-2) - np.log(depths_val + 1e-2)) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpreprocess(image):\n",
    "    img_min = -123\n",
    "    img_max = 151\n",
    "    return (image - img_min) / (img_max - img_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgbs_to_see = rgbs_val[::200]\n",
    "predictions_to_see = depths_val_predicted[::200]\n",
    "filtered_predictions_to_see = filtered_images[::200] * 10\n",
    "gts_to_see = depths_val[::200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83, 224, 224, 3) (83, 224, 224) (83, 224, 224) (83, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "print(rgbs_to_see.shape, gts_to_see.shape, predictions_to_see.shape, filtered_predictions_to_see.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(predictions_to_see)\n",
    "plt.figure(figsize=(16, n * 4))\n",
    "for i in range(30):\n",
    "    plt.subplot(n, 4, i * 4 + 1)\n",
    "    plt.title('RGB', fontsize=12)\n",
    "    plt.imshow(unpreprocess(rgbs_to_see[i]))\n",
    "    \n",
    "    plt.subplot(n, 4, i * 4 + 2)\n",
    "    plt.title('Raw prediction', fontsize=12)\n",
    "    plt.imshow(predictions_to_see[i] / 10., cmap='rainbow')\n",
    "    \n",
    "    plt.subplot(n, 4, i * 4 + 3)\n",
    "    plt.title('Filtered prediction', fontsize=12)\n",
    "    plt.imshow(filtered_predictions_to_see[i] / 10., cmap='rainbow')\n",
    "    \n",
    "    plt.subplot(n, 4, i * 4 + 4)\n",
    "    plt.title('Ground truth', fontsize=12)\n",
    "    plt.imshow(gts_to_see[i] / 10., cmap='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(predictions_to_see)\n",
    "plt.figure(figsize=(16, n * 4))\n",
    "for i in range(30, n):\n",
    "    plt.subplot(n, 4, i * 4 + 1)\n",
    "    plt.title('RGB', fontsize=12)\n",
    "    plt.imshow(unpreprocess(rgbs_to_see[i]))\n",
    "    \n",
    "    plt.subplot(n, 4, i * 4 + 2)\n",
    "    plt.title('Raw prediction', fontsize=12)\n",
    "    plt.imshow(predictions_to_see[i] / 10., cmap='rainbow')\n",
    "    \n",
    "    plt.subplot(n, 4, i * 4 + 3)\n",
    "    plt.title('Filtered prediction', fontsize=12)\n",
    "    plt.imshow(filtered_predictions_to_see[i] / 10., cmap='rainbow')\n",
    "    \n",
    "    plt.subplot(n, 4, i * 4 + 4)\n",
    "    plt.title('Ground truth', fontsize=12)\n",
    "    plt.imshow(gts_to_see[i] / 10., cmap='rainbow')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

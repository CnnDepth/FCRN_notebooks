{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение различных FCRN с шорткатами из энкодера в декодер с адаптивным лоссом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import keras\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    train_data_file.close()\n",
    "except:\n",
    "    pass\n",
    "train_data_file = h5py.File('/home/kmouraviev/NYU_dataset_hdf5/train_data_fullsize_small.hdf5')\n",
    "rgbs_train = np.array(train_data_file['data'])\n",
    "depths_train = np.array(train_data_file['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    val_data_file.close()\n",
    "except:\n",
    "    pass\n",
    "val_data_file = h5py.File('/home/kmouraviev/NYU_dataset_hdf5/validation_data_different_scenes.hdf5')\n",
    "rgbs_val = np.array(val_data_file['data'])\n",
    "depths_val = np.array(val_data_file['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41685, 384, 512, 3) (41685, 384, 512)\n",
      "(14266, 384, 512, 3) (14266, 384, 512)\n",
      "-122.68 150.061\n",
      "1.74775976467378 4.2530741218842865\n"
     ]
    }
   ],
   "source": [
    "print(rgbs_train.shape, depths_train.shape)\n",
    "print(rgbs_val.shape, depths_val.shape)\n",
    "print(rgbs_train[0].min(), rgbs_train[0].max())\n",
    "print(depths_train[0].min(), depths_train[0].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet c Deconvolution-декодером и шорткатами "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_resnet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-fc3eae8e04cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_resnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmy_batch_normalization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMyBatchNormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_resnet'"
     ]
    }
   ],
   "source": [
    "from tensorflow_resnet.my_batch_normalization import MyBatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.layers import *\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras.applications.resnet50 import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_upprojection(nn):\n",
    "    n_filters = int(nn.shape[-1])\n",
    "    batchnorm = BatchNormalization()(nn)\n",
    "    upsampling = UpSampling2D()(batchnorm)\n",
    "    conv = Conv2D(n_filters // 2, kernel_size=3, padding='same')(upsampling)\n",
    "    relu = Activation('relu')(conv)\n",
    "    conv_3 = Conv2D(n_filters // 2, kernel_size=3, padding='same')(relu)\n",
    "    conv_proj = Conv2D(n_filters // 2, kernel_size=3, padding='same')(upsampling)\n",
    "    add = Add()([conv_3, conv_proj])\n",
    "    relu_2 = Activation('relu')(add)\n",
    "    dropout = Dropout(0.5)(relu_2)\n",
    "    return dropout\n",
    "\n",
    "\n",
    "def apply_upconvolution(nn):\n",
    "    n_filters = int(nn.shape[-1])\n",
    "    batchnorm = BatchNormalization()(nn)\n",
    "    upsampling = UpSampling2D()(batchnorm)\n",
    "    conv = Conv2D(n_filters // 2, kernel_size=3, padding='same')(upsampling)\n",
    "    relu = Activation('relu')(conv)\n",
    "    dropout = Dropout(0.5)(relu)\n",
    "    return dropout\n",
    "\n",
    "\n",
    "def apply_deconvolution(nn, n_output_filters):\n",
    "    batchnorm = MyBatchNormalization()(nn, training=None)\n",
    "    deconv = Conv2DTranspose(n_output_filters,\n",
    "                             kernel_size=(5, 5),\n",
    "                             strides=(2, 2),\n",
    "                             padding='same',\n",
    "                             output_padding=1\n",
    "                            )(batchnorm)\n",
    "    relu = Activation('relu')(deconv)\n",
    "    dropout = Dropout(0.5)(relu)\n",
    "    return dropout\n",
    "\n",
    "\n",
    "def apply_nonbt_1d(nn, n_output_filters, k):\n",
    "    conv1 = Conv2D(n_output_filters, \n",
    "                   kernel_size=(3, 1), \n",
    "                   padding='same', \n",
    "                   name='nonbt{}_conv1'.format(k)\n",
    "                  )(nn)\n",
    "    conv2 = Conv2D(n_output_filters,\n",
    "                   kernel_size=(1, 3),\n",
    "                   padding='same',\n",
    "                   name='nonbt{}_conv2'.format(k)\n",
    "                  )(conv1)\n",
    "    bn1 = MyBatchNormalization()(conv2, training=None, name='bn_nonbth_' + str(k))\n",
    "    conv3 = Conv2D(n_output_filters,\n",
    "                   kernel_size=(3, 1),\n",
    "                   padding='same',\n",
    "                   name='nonbt{}_conv3'.format(k)\n",
    "                  )(bn1)\n",
    "    conv4 = Conv2D(n_output_filters, \n",
    "                   kernel_size=(1, 3),\n",
    "                   padding='same',\n",
    "                   name='nonbt{}_conv4'.format(k)\n",
    "                  )(conv3)\n",
    "    return conv4\n",
    "\n",
    "\n",
    "def apply_upconv_nonbt(nn, n_output_filters):\n",
    "    batchnorm = BatchNormalization()(nn)\n",
    "    upsampling = UpSampling2D()(batchnorm)\n",
    "    nonbt = apply_nonbt_1d(upsampling, n_output_filters)\n",
    "    relu = Activation('relu')(nonbt)\n",
    "    return relu\n",
    "    \n",
    "\n",
    "def create_fcrn_model(encoder, deconv_type='projection', h=224, w=224, use_nonbt_blocks=False):\n",
    "    for layer in encoder.layers:\n",
    "        layer.trainable = True\n",
    "    for layer in encoder.layers:\n",
    "        if layer.name == 'my_batch_normalization_1':\n",
    "            conv1 = layer.output\n",
    "        if layer.name == 'add_3':\n",
    "            add3 = layer.output\n",
    "        if layer.name == 'add_7':\n",
    "            add7 = layer.output\n",
    "        if layer.name == 'add_13':\n",
    "            add13 = layer.output\n",
    "    print(add3.shape, add7.shape, add13.shape)\n",
    "    encoder.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    resnet_output = encoder.output\n",
    "    nn = Conv2D(1024, kernel_size=1, padding='same', name='starting_conv')(resnet_output)\n",
    "    for k in range(5):\n",
    "        if k == 1:\n",
    "            nn = concatenate([add13, nn], axis=-1)\n",
    "        if k == 2:\n",
    "            nn = concatenate([add7, nn], axis=-1)\n",
    "        if k == 3:\n",
    "            nn = concatenate([add3, nn], axis=-1)\n",
    "        if k == 4:\n",
    "            nn = concatenate([conv1, nn], axis=-1)\n",
    "        if k == 0:\n",
    "            n_filters = int(nn.shape[-1]) // 2\n",
    "        elif k == 1 or k == 4:\n",
    "            n_filters = int(nn.shape[-1]) // 3\n",
    "        else:\n",
    "            n_filters = int(nn.shape[-1]) // 4\n",
    "        if deconv_type == 'projection':\n",
    "            nn = apply_upprojection(nn, n_filters)\n",
    "        if deconv_type ==  'convolution':\n",
    "            nn = apply_upconvolution(nn, n_filters)\n",
    "        if deconv_type == 'deconvolution':\n",
    "            nn = apply_deconvolution(nn, n_filters)\n",
    "            if use_nonbt_blocks and k > 0 and k < 4:\n",
    "                nn = apply_nonbt_1d(nn, n_filters)\n",
    "        if deconv_type == 'conv_nonbt':\n",
    "            nn = apply_upconv_nonbt(nn, n_filters, k)\n",
    "    depth_output = Conv2D(1, kernel_size=3, padding='same', name='final_conv')(nn)\n",
    "    depth_output = Activation('relu', name='final_relu')(depth_output)\n",
    "    depth_output = Reshape((h, w))(depth_output)\n",
    "    fcrn_model = Model(inputs=encoder.input, outputs=depth_output)\n",
    "    for layer in fcrn_model.layers:\n",
    "        layer.trainable = True\n",
    "    return fcrn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "encoder = load_model('models/resnet50/resnet_encoder_coco.hdf5',\n",
    "                     custom_objects={'MyBatchNormalization': MyBatchNormalization})\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        model = create_fcrn_model(encoder, deconv_type='conv_nonbt', h=384, w=512)\n",
    "        model.summary()\n",
    "    except:\n",
    "        continue\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение на малом датасете с адаптивным лоссом с использованием Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_utils.training import train_model\n",
    "from experiment_utils.callbacks import LoggingCallback\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gpu = multi_gpu_model(model, gpus=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = tf.Variable(1.0)\n",
    "delta = tf.constant(0.1)\n",
    "delta_lr = tf.constant(1e-2)\n",
    "labels_plh = tf.placeholder(tf.float32, shape=(None, 384, 512))\n",
    "predictions = model_gpu.output\n",
    "\n",
    "# define k adjusting\n",
    "error = tf.abs(labels_plh - predictions)\n",
    "pixels_above_k = tf.cast(tf.logical_and(labels_plh > k, labels_plh < k + delta), tf.float32)\n",
    "pixels_below_k = tf.cast(tf.logical_and(labels_plh < k, labels_plh > k - delta), tf.float32)\n",
    "error_above_k = tf.reduce_sum(pixels_above_k * error) / tf.reduce_sum(pixels_above_k)\n",
    "error_below_k = tf.reduce_sum(pixels_below_k * error) / tf.reduce_sum(pixels_below_k)\n",
    "dk = tf.where(error_above_k > error_below_k,\n",
    "              delta * delta_lr,\n",
    "              -delta * delta_lr)\n",
    "change_k = k.assign(tf.maximum(k + dk, delta))\n",
    "\n",
    "# define loss: berhu on pixels < k, mse on pixels > k\n",
    "berhu_threshold = tf.minimum(0.2 * tf.reduce_max(error), 0.5)\n",
    "berhu_loss = tf.where(error < berhu_threshold,\n",
    "                      error, \n",
    "                      (error ** 2 + berhu_threshold ** 2) / (2 * berhu_threshold))\n",
    "square_loss = error ** 2\n",
    "loss = tf.where(labels_plh < k,\n",
    "                berhu_loss,\n",
    "                square_loss)\n",
    "loss_mean = tf.reduce_mean(loss)\n",
    "mse = tf.reduce_mean(square_loss)\n",
    "\n",
    "# define optimizer\n",
    "optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "train_step = optimizer.minimize(loss_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_initializers = [var.initializer for var in optimizer.variables()]\n",
    "session.run(adam_initializers)\n",
    "session.run(k.initializer)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(tf.local_variables_initializer())\n",
    "model.load_weights('fcrn_starting_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, n):\n",
    "    cumsum = np.cumsum(x)\n",
    "    return (cumsum[n:] - cumsum[:-n]) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 4\n",
    "batch_size = 16\n",
    "verbose_freq = 10\n",
    "loss_history = []\n",
    "mse_history = []\n",
    "k_history = []\n",
    "for epoch in range(n_epochs):\n",
    "    ids = np.arange(len(rgbs_train))\n",
    "    np.random.shuffle(ids)\n",
    "    for i in range(0, len(rgbs_train), batch_size):\n",
    "        rgbs_batch = rgbs_train[ids[i:min(i + batch_size, len(rgbs_train))]]\n",
    "        depths_batch = depths_train[ids[i:min(i + batch_size, len(rgbs_train))]]\n",
    "        k_value, _, loss_value, mse_value, __ = session.run([k, change_k, loss_mean, mse, train_step],\n",
    "                                                 feed_dict={model_gpu.input: rgbs_batch,\n",
    "                                                            labels_plh: depths_batch\n",
    "                                                           }\n",
    "                                                )\n",
    "        loss_history.append(loss_value)\n",
    "        mse_history.append(mse_value)\n",
    "        k_history.append(k_value)\n",
    "        if i % verbose_freq == 0:\n",
    "            clear_output()\n",
    "            # print loss and k\n",
    "            print('average loss over last {} batches: {}'.format(verbose_freq, np.mean(loss_history[-10:])))\n",
    "            print('average MSE over last {} batches: {}'.format(verbose_freq, np.mean(mse_history[-10:])))\n",
    "            print('k: {}'.format(k_history[-1]))\n",
    "            \n",
    "            if i > 100:\n",
    "                # plot loss and mse\n",
    "                plt.figure(figsize=(16, 16))\n",
    "                plt.subplot(2, 1, 1)\n",
    "                plt.plot(running_mean(loss_history, 100), label='loss')\n",
    "                plt.plot(running_mean(mse_history, 100), label='MSE')\n",
    "                plt.legend(fontsize=16)\n",
    "                plt.grid(ls=':')\n",
    "                \n",
    "                # plot k\n",
    "                plt.subplot(2, 2, 1)\n",
    "                plt.plot(running_mean(k_history, 100), label='k')\n",
    "                plt.legend(fontsize=16)\n",
    "                plt.grid(ls=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
